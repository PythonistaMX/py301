{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![imagenes/pythonista.png](imagenes/pythonista.png)](https://pythonista.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo.\n",
    "\n",
    "El objetivo de este capítulo es ejemplificar el uso de herramientas como *NLTK* y *networkX* en un ejemplo práctico.\n",
    "\n",
    "EL código siguiente definir;a algunas funciones capaces de:\n",
    "\n",
    "* Conectarse a la API de Twitter.\n",
    "* Realizar búsquedas a partir de un términoy obtener el resultado en formato JSON.\n",
    "* Guardar y recuperar en un archivo JSON diversos tuits.\n",
    "* Crear una grafo que relaciona a usuarios con sus tuits.\n",
    "* Normalizar los textos de los tuits.\n",
    "* Crear una nube de palabras a partir de los textos de los tuis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones iniciales.\n",
    "\n",
    "* Se instalarán los compomentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk networkx twitter wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se importarán las herramientas necesarias para ejecutar el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, twitter, json, nltk\n",
    "from functools import reduce\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se inicializarán las credenciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_SECRET = \"\", \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función *acceede_a_tw()*.\n",
    "\n",
    "* Extraerá las credenciales de la API de Twitter a partir de un achivo cuya ruta es definida como argumento.\n",
    "* Regresará un objeto que contiene una conexión a la API de Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accede_a_tw(fuente):\n",
    "    (API_KEY,\n",
    "     API_SECRET,\n",
    "     ACCESS_TOKEN,\n",
    "     ACCESS_SECRET) = open(\n",
    "            fuente, 'r').read().splitlines()\n",
    "    auth = twitter.oauth.OAuth(ACCESS_TOKEN,\n",
    "                           ACCESS_SECRET,\n",
    "                           API_KEY,\n",
    "                           API_SECRET)\n",
    "    return twitter.Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función *busqueda_tw()*.\n",
    "\n",
    "Esta función realizará la búsqueda de un término mediante la API de Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_tw(tw, termino):\n",
    "    return tw.search.tweets(q=termino, lang=\"es\", count=\"500\")[\"statuses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_lista(archivo):\n",
    "    try:\n",
    "        with open(archivo, \"r\") as f:\n",
    "            lista = [elemento.replace(\" \", \"\") for elemento in \\\n",
    "                     reduce(lambda x, y: x + y,\n",
    "                            [linea for linea in csv.reader(f, dialect=\"unix\")])]\n",
    "    except IOError:\n",
    "        lista = []\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_tuits(archivo):\n",
    "    try:\n",
    "        f = open(archivo, \"r\")\n",
    "        resultado = json.load(f)\n",
    "    except IOError:\n",
    "        resultado = []\n",
    "    else:\n",
    "        f.close()\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guarda_tuits(tuits, archivo):\n",
    "    with open(archivo, \"w\") as f:\n",
    "        json.dump(tuits, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mezcla_tuits(actuales, nuevos):\n",
    "    for tuit in nuevos: \n",
    "        if tuit[\"id\"] not in [actual[\"id\"] for actual in actuales]:\n",
    "            actuales.append(tuit)\n",
    "    return actuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar(texto):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    limpio = tokenizer.tokenize(texto)    \n",
    "    return limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analiza_menciones(tuits):\n",
    "    pares = []\n",
    "    nodos = []\n",
    "    for tuit in tuits:\n",
    "        usuario = tuit[\"user\"][\"screen_name\"]\n",
    "        nodos.append(usuario)\n",
    "        menciones = [mencion[\"screen_name\"] for mencion in tuit[\"entities\"][\"user_mentions\"]]\n",
    "        for mencion in menciones:\n",
    "            if mencion != [] and usuario != mencion:\n",
    "                par = (usuario, mencion)\n",
    "                pares.append(par)\n",
    "    nodos = list(set(nodos))\n",
    "    pares = list(set(pares))\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodos)\n",
    "    G.add_edges_from(pares)\n",
    "    plt.figure(figsize=(32,32))\n",
    "    nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refina_texto(tuits, lista, termino):\n",
    "    lista_negra = carga_lista(lista) + [palabra.replace(\"@\", \"\") for palabra in termino.split()]\n",
    "    texto =\"\"\n",
    "    for i in range(0, len(lista_negra)):\n",
    "        lista_negra[i] = lista_negra[i].lower()\n",
    "    for tuit in tuits:\n",
    "        texto += (tuit[\"text\"] + \" \")\n",
    "        depurador = nltk.RegexpTokenizer(r'\\w+')\n",
    "    limpio = depurador.tokenize(texto)\n",
    "    texto = \"\"\n",
    "    for termino in limpio:\n",
    "        termino = termino.lower()\n",
    "        if  termino not in lista_negra:\n",
    "            texto += (termino + \" \") \n",
    "    return str(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nube(texto):\n",
    "    wordcloud = WordCloud().generate(texto)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(archivo=\"tuits.json\", lista=\"data/lista_negra.csv\"): \n",
    "    termino = input(\"Término de búsqueda: \")\n",
    "    tuits_previos = carga_tuits(archivo)\n",
    "    tw = accede_a_tw(\"data/credenciales.txt\")\n",
    "    tuits_recientes = busqueda_tw(tw, termino)\n",
    "    tuits = mezcla_tuits(tuits_previos, tuits_recientes)\n",
    "    guarda_tuits(tuits, archivo)\n",
    "    analiza_menciones(tuits)\n",
    "    return refina_texto(tuits, lista, termino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nube(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuits = carga_tuits(\"tuits.json\")\n",
    "texto = refina_texto(tuits, \"data/lista_negra.csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nube(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2019.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
